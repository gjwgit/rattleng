**Decision Trees**

A **decision tree** is the prototypical data mining tool used widely
for its ease of interpretation. 

A decision tree consists of a root node where the data is split into
two (for a binary tree) smaller datasets using a test on a single
variable. In turn, these two smaller dataset represent new nodes of
the tree that may then each be further split on a single (and usually
different) variable. This divide and conquering continues until no
further splitting would improve the performance of the model.

While a choice of measures are available to select a variable to split
the dataset on, the Gini measure is used, and generally is no
different to the information measure for binary classification. To
explore the alternatives, copy the relevant code from the Log and
paste it into the R Console and change any of the options.

Common options that a user may change from their default values are
available.

Priors: used to boost a particularly important class, by giving it a
higher prior probability. Expects a list of numbers that sum up to 1,
and of the same length as the number of classes in the training dataset:
e.g.,0.5,0.5.

Loss Matrix: used to weight the outcome classes differently:
e.g., 0,10,1,0.

Other
options exist, but are not usually required. For example, 10-fold
cross validation, used in deciding how to prune to the best decision
tree, is generally regarded as the right number. Transferring the
commands from the Log tab into the R Console does give you full access
to all options.

Decision trees work with both numeric and categoric data.

R's [rpart](https://www.rdocumentation.org/packages/rpart) package is
used to build the decision tree.
